{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ebfc8e7"
      },
      "outputs": [],
      "source": [
        "#Uncomment these if running in colab\n",
        "!pip install wget\n",
        "!pip install newspaper3k\n",
        "!pip install xmltodict\n",
        "!pip install pandarallel\n",
        "!pip install datefinder\n",
        "!pip install pydrive\n",
        "!pip install selenium\n",
        "!pip3 install \"urllib3 <=1.26.15\"\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jme9mrBILUy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lahym2YQm8ff"
      },
      "source": [
        "# <font color='red'>If imports give you any errors, uncomment the install cell above.</font>\n",
        "### You only need to install these libraries once.\n",
        "### Future runs or scripts, you can comment them out to save time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5314e252",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import wget\n",
        "import newspaper\n",
        "from newspaper import Article\n",
        "import json\n",
        "import pandas as pd\n",
        "pd.set_option(\"max_colwidth\", 600)\n",
        "import ast\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import requests\n",
        "import time\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import os\n",
        "import html\n",
        "import re\n",
        "import itertools\n",
        "import lxml\n",
        "import xmltodict\n",
        "import collections\n",
        "from urllib import request\n",
        "from collections import OrderedDict\n",
        "from urllib.error import HTTPError, URLError\n",
        "from urllib.parse import urlparse\n",
        "import sys\n",
        "import ast\n",
        "import time\n",
        "from pandarallel import pandarallel\n",
        "import requests\n",
        "import datefinder\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import psutil\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException\n",
        "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
        "from multiprocessing import Pool\n",
        "import random\n",
        "from urllib.parse import quote\n",
        "import pytz\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnMBfSuRm8fh"
      },
      "source": [
        "# <font color='red'>The next cell you need to update for each site you scrape </font>\n",
        "# <font color='red'>I recommend creating a sperate script for each site you scrape. </font>\n",
        "# <font color='red'>Test scraping of few pages, to check for errors, before scraping all site </font>\n",
        "\n",
        "\n",
        "### > Name the script above from HPC NEW Scraper to the site name.\n",
        "\n",
        "### > Please remember to fill/update news outlet and country for site\n",
        "\n",
        "### > Remember to change \"types\" depending on page if it's numbered by pages, or click more or scroll down [0,1,2]\n",
        "### > max_click_SHOW_MORE = the number of clicks or scroll down you like.\n",
        "\n",
        "\n",
        "### > extract_text_sleep = sleep amount for extracting text\n",
        "### > extract_urls_sleep = sleep amount for extracting links\n",
        "\n",
        "### > Remmember to update page identifer on line 39 if website is a pages site\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e7001a6"
      },
      "outputs": [],
      "source": [
        "t0 = time.time()\n",
        "\n",
        "links_file_path = \"\"\n",
        "csv_file_path = \"\"\n",
        "\n",
        "#PLEASE EDIT THIS CELL!\n",
        "\n",
        "#Universal parameters. Always needs to be changed\n",
        "news_outlet = \"\"\n",
        "country = \"\"\n",
        "max_click_SHOW_MORE = 500\n",
        "host = \"\"\n",
        "date_format = \"%Y-%m-%d\"\n",
        "relevant_path = f'^{host}.*$'\n",
        "# Allow http and https links\n",
        "if re.match(\"^https?.*\", host):\n",
        "  relevant_path = f'^https?:\\/\\/{host[8:]}.*$'\n",
        "else:\n",
        "  relevant_path = f'^https?:\\/\\/{host}.*$'\n",
        "urls = ['/'] #if section url has arabic use \"quote(/news/سياسة)\"\n",
        "urls = [host + url for url in urls]\n",
        "csv_name = country+'_'+news_outlet+'.csv'\n",
        "types = ['page', 'click_more','scroll_down']\n",
        "type_of_page = types[0]\n",
        "extract_text_sleep = random.randint(10,12)\n",
        "extract_urls_sleep = random.randint(10,12)\n",
        "\n",
        "\n",
        "# If page type is click_more or scroll_down i.e. can get more article by clicking a button with text 'click more' or simply scrolling down\n",
        "#-----------------------------------------------------------------------------------------------------------------------------------------\n",
        "xpath_for_link = \"\"\n",
        "xpath_for_click_more = \"\"\n",
        "#-----------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# If page type is page\n",
        "#-----------------------------------------------------------------------------------------------------------------------------------------\n",
        "target_tag = ''\n",
        "target_tag_class = \"\"\n",
        "\n",
        "page_identifier = ''\n",
        "\n",
        "if type_of_page == 'page':\n",
        "    urls = [url + page_identifier for url in urls]\n",
        "pages_each_category = [1] #The total number of pages for each section. So if policiaca is 1..92, write 92, not 93.\n",
        "if pages_each_category:\n",
        "    total_pages = pages_each_category\n",
        "else:\n",
        "    total_pages = 500\n",
        "#-----------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "#Get amount of pages per category not having text code\n",
        "#-----------------------------------------------------------------------------------------------------------------------------------------\n",
        "retrieve_amount_of_pages_no_text = False\n",
        "#-----------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "#Get amount of pages per category with text code\n",
        "#-----------------------------------------------------------------------------------------------------------------------------------------\n",
        "retrieve_amount_of_pages_w_text = False\n",
        "xpath_for_amount_of_pages = \"\"\n",
        "index_for_amount_of_pages_location = 0\n",
        "#-----------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# in case multi tags\n",
        "target_tags = []\n",
        "target_tag_classes = []\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPVY5bHd6gM5"
      },
      "outputs": [],
      "source": [
        "def parallelize_dataframe(df, func, n_cores):\n",
        "    df_split = np.array_split(df, n_cores)\n",
        "    pool = Pool(n_cores)\n",
        "    df = pd.concat(pool.map(func, df_split))\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions for automatically identifying the number of pages\n",
        "\n",
        "def check_it_has_links(url):\n",
        "  options = FirefoxOptions()\n",
        "  options.add_argument(\"--headless\")\n",
        "  driver = webdriver.Firefox(options=options)\n",
        "  driver.get(url)\n",
        "\n",
        "  has_links= True\n",
        "  try:\n",
        "    element = WebDriverWait(driver, 5).until(\n",
        "        EC.presence_of_element_located((By.XPATH, xpath_for_link))\n",
        "    )\n",
        "  except TimeoutException as e:\n",
        "    print('Could not find pages count')\n",
        "    has_links = False\n",
        "  driver.quit()\n",
        "  return has_links\n",
        "\n",
        "def retrieve_pages_per_category_no_text(url):\n",
        "  pages = 1\n",
        "  pages_defined = False\n",
        "\n",
        "  while check_it_has_links(url + f'{pages}'):\n",
        "    pages = pages * 10\n",
        "\n",
        "  add_pages = pages // 2\n",
        "  if pages != 1:\n",
        "    pages = pages // 2\n",
        "    while not pages_defined:\n",
        "      add_pages = add_pages // 2\n",
        "      if check_it_has_links(url + f'{pages}'):\n",
        "        pages = add_pages + pages\n",
        "      else:\n",
        "        pages = pages - add_pages\n",
        "      if add_pages == 1:\n",
        "        pages_defined = True\n",
        "      print(pages)\n",
        "\n",
        "  return pages\n",
        "\n",
        "def retrieve_pages_per_category_w_text(url):\n",
        "  options = FirefoxOptions()\n",
        "  options.add_argument(\"--headless\")\n",
        "  driver = webdriver.Firefox(options=options)\n",
        "  driver.get(url)\n",
        "  try:\n",
        "    element = WebDriverWait(driver, 10).until(\n",
        "        EC.presence_of_element_located((By.XPATH, xpath_for_amount_of_pages))\n",
        "    )\n",
        "  except TimeoutException as e:\n",
        "    print('Could not find pages count')\n",
        "    return ''\n",
        "  pages = driver.find_element(By.XPATH, xpath_for_amount_of_pages).text\n",
        "  if not pages:\n",
        "    print('No text found')\n",
        "  driver.quit()\n",
        "  return pages"
      ],
      "metadata": {
        "id": "sTpYRBWi6nkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if retrieve_amount_of_pages_no_text:\n",
        "  total_pages = []\n",
        "  for url in urls:\n",
        "    amount_of_pages = retrieve_pages_per_category_no_text(url)\n",
        "    total_pages.append(amount_of_pages)\n",
        "  print(total_pages)\n",
        "\n",
        "if retrieve_amount_of_pages_w_text:\n",
        "  total_pages = []\n",
        "  for url in urls:\n",
        "    text_with_amount_of_pages = retrieve_pages_per_category_w_text(url)\n",
        "\n",
        "    amount_of_pages =[]\n",
        "\n",
        "    for char in text_with_amount_of_pages[index_for_amount_of_pages_location:]:\n",
        "      if not char.isnumeric():\n",
        "        if char != ',' and char != '.':\n",
        "          break\n",
        "      else:\n",
        "        amount_of_pages.append(char)\n",
        "    amount_of_pages = ''.join(amount_of_pages)\n",
        "    print(amount_of_pages + ' ' + url)\n",
        "    if amount_of_pages:\n",
        "      total_pages.append(int(amount_of_pages))\n",
        "    else:\n",
        "      total_pages.append(1)\n",
        "  print(total_pages)"
      ],
      "metadata": {
        "id": "Dh01uZzU672j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9cf2819"
      },
      "outputs": [],
      "source": [
        "\n",
        "output_path = 'links_'+csv_name\n",
        "\n",
        "def retrieve_links_from_list(url,scroll_down=False):\n",
        "  options = FirefoxOptions()\n",
        "  options.add_argument(\"--headless\")\n",
        "  driver = webdriver.Firefox(options=options)\n",
        "  driver.get(url)\n",
        "\n",
        "  count = 0\n",
        "\n",
        "  news_urls = set()\n",
        "\n",
        "  prev_length = 0\n",
        "  while count <= max_click_SHOW_MORE:\n",
        "      if not scroll_down:\n",
        "          try:\n",
        "            element = WebDriverWait(driver, 10).until(\n",
        "                EC.presence_of_element_located((By.XPATH, xpath_for_click_more))\n",
        "            )\n",
        "          except TimeoutException as e:\n",
        "            print('Could not find click more button')\n",
        "            break\n",
        "\n",
        "\n",
        "      try:\n",
        "          all_links = driver.find_elements(By.XPATH, xpath_for_link)\n",
        "          if not scroll_down:\n",
        "            click_more = driver.find_element(By.XPATH, xpath_for_click_more)\n",
        "          curr_length = len(all_links)\n",
        "          print(f'{curr_length} links in current page')\n",
        "          print(f'{prev_length} links in previous page')\n",
        "          if count > 0:\n",
        "            if curr_length == prev_length or (not scroll_down and not click_more):\n",
        "              break\n",
        "\n",
        "          current_links = [l.get_attribute(\"href\") for l in all_links[prev_length:]]\n",
        "          df_link = pd.DataFrame(current_links, columns = ['link'])\n",
        "          df_link.to_csv(output_path, mode='a', header=not os.path.exists(output_path))\n",
        "\n",
        "          prev_length = curr_length\n",
        "\n",
        "          if not scroll_down:\n",
        "\n",
        "            try:\n",
        "                click_more.click();\n",
        "            except Exception as e:\n",
        "                driver.execute_script(\"arguments[0].click();\", click_more) #If click does not work because of overlapping elements, this executes\n",
        "\n",
        "            print(f\"Button clicked {count} times\", )\n",
        "          else:\n",
        "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "            print(f\"Scrolled down {count+1} times\")\n",
        "\n",
        "          time.sleep(3)\n",
        "\n",
        "          count += 1\n",
        "\n",
        "\n",
        "\n",
        "      except TimeoutException:\n",
        "          break\n",
        "\n",
        "  time.sleep(2)\n",
        "\n",
        "  all_links = driver.find_elements(By.XPATH, xpath_for_link)\n",
        "\n",
        "  for link in all_links:\n",
        "        if isinstance(link.get_attribute('href'), str):\n",
        "          if re.match(relevant_path, link.get_attribute('href')):\n",
        "            news_urls.add(link.get_attribute('href'))\n",
        "\n",
        "  driver.quit()\n",
        "  print('*' * 20)\n",
        "  return list(news_urls)\n",
        "\n",
        "\n",
        "def extract_urls(url):\n",
        "    rows = set()\n",
        "    soup = getSoup(url)\n",
        "    if soup:\n",
        "      all_elems = None\n",
        "      if len(target_tags) > 0 and len(target_tag_classes) > 0:\n",
        "        all_elems = []\n",
        "        for tag in target_tags:\n",
        "          for tag_class in target_tag_classes:\n",
        "            all_elems += soup.find_all(tag, {'class':tag_class})\n",
        "      elif target_tag_class != '':\n",
        "        all_elems = soup.find_all(target_tag, {'class':target_tag_class})\n",
        "      else:\n",
        "        all_elems = soup.find_all(target_tag)\n",
        "\n",
        "      for d in all_elems:\n",
        "        all_links = d.find_all('a', href=True)\n",
        "        if not all_links:\n",
        "            all_links = [d]\n",
        "        for l in all_links:\n",
        "            if not re.match('^http.*', l['href']):\n",
        "              l['href'] = host + l['href']\n",
        "\n",
        "            if re.match(relevant_path, l['href']):\n",
        "              rows.add(l['href'])\n",
        "\n",
        "\n",
        "    if rows:\n",
        "      return list(rows)\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "def extract_urls_from_df(df):\n",
        "    links = df['page'].map(lambda x: extract_urls(x))\n",
        "    df['link'] = links\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ecec80d"
      },
      "outputs": [],
      "source": [
        "pages = []\n",
        "\n",
        "def get_zipped_urls(url, total_pages):\n",
        "    page_numbers = [str(i) for i in range(1,total_pages)]\n",
        "    url_multiple = [url] * total_pages\n",
        "    return [''.join(x) for x in zip(url_multiple,page_numbers)]\n",
        "\n",
        "if type_of_page == 'page':\n",
        "    for ind, url in enumerate(urls):\n",
        "        if isinstance(total_pages, int):\n",
        "            pages += get_zipped_urls(url, total_pages)\n",
        "        else:\n",
        "            pages_for_category = total_pages[ind] + 1\n",
        "            pages += get_zipped_urls(url, pages_for_category)\n",
        "\n",
        "elif type_of_page == 'click_more' or type_of_page == 'scroll_down':\n",
        "    for url in urls:\n",
        "        pages += retrieve_links_from_list(url, True if type_of_page == 'scroll_down' else False)\n",
        "\n",
        "\n",
        "df = pd.DataFrame(pages, columns = ['page'])\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0755f971"
      },
      "outputs": [],
      "source": [
        "def getSoup(url):\n",
        "    \"\"\"\n",
        "    Return a soup object of the URL\n",
        "    \"\"\"\n",
        "    try:\n",
        "        req = request.Request(url, headers={'User-Agent' : \"Chrome\"})\n",
        "        con = request.urlopen(req)\n",
        "        time.sleep(extract_urls_sleep)\n",
        "        html = con.read()\n",
        "\n",
        "    except HTTPError as e:\n",
        "        print(e)\n",
        "        return None\n",
        "\n",
        "    except URLError as e:\n",
        "        print('The server could not be found')\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      return None\n",
        "\n",
        "\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    return soup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78105dd7"
      },
      "outputs": [],
      "source": [
        "#New Code caused issues\n",
        "\n",
        "def extract_text(url):\n",
        "    text, title, date = None, None, None\n",
        "    try:\n",
        "        result = [country, news_outlet] + [''] * 3\n",
        "        article = Article(url, keep_article_html=False)\n",
        "        article.download()\n",
        "        time.sleep(extract_text_sleep)\n",
        "        article.parse()\n",
        "        text = article.text\n",
        "\n",
        "        if text:\n",
        "            text_copy = text\n",
        "            title = article.title\n",
        "            date = article.publish_date\n",
        "            if date:\n",
        "                date = date.strftime(date_format)\n",
        "            else:\n",
        "                matches = datefinder.find_dates(text_copy)\n",
        "                most_recent_datetime = sorted(matches)[-1]\n",
        "                date = most_recent_datetime.strftime(date_format)\n",
        "    finally:\n",
        "      if title:\n",
        "        result[2] = title\n",
        "      if date:\n",
        "        result[3] = date\n",
        "      if text:\n",
        "        result[4] = text\n",
        "      return result\n",
        "\n",
        "\n",
        "def extract_text_from_df(df):\n",
        "    content = df['link'].map(lambda x: extract_text(x))\n",
        "    df['content'] = content\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f114c52c"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83ebc4cd"
      },
      "outputs": [],
      "source": [
        "def parallel_work(df, method_to_run, target, source):\n",
        "  global workers\n",
        "  while workers>0:\n",
        "    try:\n",
        "      df[target] = df[source].parallel_apply(method_to_run)\n",
        "    except Exception as e:\n",
        "      raise e\n",
        "    break\n",
        "\n",
        "  if workers == 0:\n",
        "    print('Error during parallel operation. Could not extract text')\n",
        "\n",
        "  return df\n",
        "\n",
        "def get_parallel_operation_results(divided_dfs, method_to_run, target, source):\n",
        "  res = []\n",
        "  for df in divided_dfs:\n",
        "    try:\n",
        "      temp_df = parallel_work(df, method_to_run, target, source)\n",
        "      if temp_df[target].isnull().all():\n",
        "        print('Could not retrieve any URLS')\n",
        "        print('Something is wrong with the target_tag and target_tag_class variables. Please modify')\n",
        "        return []\n",
        "      res.append(temp_df)\n",
        "    except Exception:\n",
        "      continue\n",
        "  df_result = pd.concat(res)\n",
        "  return df_result\n",
        "\n",
        "def partition_df(df):\n",
        "  global articles_per_parallel_operation\n",
        "  divided_dfs = []\n",
        "  start = 0\n",
        "  while start < len(df):\n",
        "    divided_dfs.append(df[start:start+articles_per_parallel_operation])\n",
        "    start += articles_per_parallel_operation\n",
        "  return divided_dfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ada4c2eb"
      },
      "outputs": [],
      "source": [
        "if type_of_page == 'page':\n",
        "    start = 0\n",
        "    limit = 40\n",
        "    total_time_start = time.time()\n",
        "    results = []\n",
        "\n",
        "    while start < len(df):\n",
        "        start_time = time.time()\n",
        "        results.append(parallelize_dataframe(df[start:start+limit], extract_urls_from_df, 94)) ## NUMBER of CORES here\n",
        "        end_time = time.time()\n",
        "        print(f'Batch of data of row range {start}-{start+limit} complete in {round(end_time-start_time, 2)} seconds')\n",
        "        print(f'{round(min((((start+limit) / len(df)) * 100), 100), 2)}% complete')\n",
        "        start+=limit\n",
        "\n",
        "    df = pd.concat(results)\n",
        "    total_time_end = time.time()\n",
        "    print(f'total time taken: {round(total_time_end - total_time_start,2)} second')\n",
        "\n",
        "else:\n",
        "    df = df.rename(columns={\"page\": \"link\"})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWwwO-9jm8fk"
      },
      "source": [
        "# <font color='red'> If you're having problems extracting links (HTTP 500, 503, etc) </font>\n",
        "\n",
        "### > Double check in browser the links from 'page' column below to see if they're correct.\n",
        "### > If they're not working, correct your host, url and page identifier you filled in above.\n",
        "\n",
        "## <font color='red'> If links are correct </font>\n",
        "### > Double check your html tags that reference article urls.\n",
        "### > Try other tags that reference the article link, such as links embedded in article picture or a separate read more button that leads to article page\n",
        "\n",
        "## <font color='red'> If tags correct </font>\n",
        "### > Increase the sleep time of extract_urls_sleep in set up cell.\n",
        "### > So if it was random.randint(5,7), set it to random.randint(10,12)\n",
        "\n",
        "\n",
        "## <font color='red'> if issue still persist </font>\n",
        "### > Try to reduce number of cores in above cell from 94 to a lower number\n",
        "### > For example, if you're getting HTTP 500 errors with 94 cores, I will lower it down to 50. If that doesn't work 25 and so on!\n",
        "\n",
        "# <font color='red'> If all that doesn't work and site is blocking you, just move on to next site and list reason/issue in excel file :) </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHVfbOYU5MtL"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f604cbb"
      },
      "outputs": [],
      "source": [
        "df.to_csv(links_file_path+csv_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BR5D1Xpn2w6G"
      },
      "outputs": [],
      "source": [
        "print(len(df))\n",
        "df = df[df['link'].notna()]\n",
        "df = df[(df['link'].str.len() != 0)]\n",
        "print(len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c1ca560",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "if type_of_page == 'page':\n",
        "    df = df.explode('link')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDqQExlIZ-YT"
      },
      "outputs": [],
      "source": [
        "print(len(df))\n",
        "df = df.drop_duplicates(subset='link')\n",
        "print(len(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33tU5CUVm8fl"
      },
      "source": [
        "# <font color='red'> IMPORTANT - UPDATE LINE 4 NEXT CELL WITH YOUR \"links\" SAVE DIRECTORY </font>\n",
        "\n",
        "## Update this Path below to reflect your special workspace '/xdisk/josorio1/conflibert_esp/Summer_2023/Sultan/links/\n",
        "\n",
        "## >> For SPANISH <<\n",
        "### Go to https://ood.hpc.arizona.edu/pun/sys/dashboard/files/fs//xdisk/josorio1/conflibert_esp/Summer_2023\n",
        "### Find your directory and create folder named 'links' inside of it.\n",
        "### Go inside 'links' folder and click copy path and paste it below.\n",
        "### Make sure you close your copied path with \"/\"\n",
        "\n",
        "## >> For Arabic <<\n",
        "### Go to https://ood.hpc.arizona.edu/pun/sys/dashboard/files/fs//xdisk/josorio1/conflibert_ara/Summer_2023\n",
        "### Find your directory and create folder named 'links' inside of it.\n",
        "### Go inside 'links' folder and click copy path and paste it below.\n",
        "### Make sure you close your copied path with \"/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80bc38c9"
      },
      "outputs": [],
      "source": [
        "#PLEASE UPDATE\n",
        "#Create a links checkpoint csv\n",
        "df.to_csv('links_'+csv_name)\n",
        "df.to_csv(links_file_path+'links_'+csv_name, encoding = 'utf-8-sig')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnp-nQgdfpJA"
      },
      "outputs": [],
      "source": [
        "if 'df' not in locals() or df is None or 'link' not in df:\n",
        "    if(os.path.exists('links_'+csv_name)):\n",
        "        df = pd.read_csv('links_'+csv_name, index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToQVNKdf8eD4"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v73KqFYTeKrz"
      },
      "outputs": [],
      "source": [
        "output_path= 'unprocessed_' + csv_name\n",
        "start = 0\n",
        "limit = 40\n",
        "\n",
        "res = []\n",
        "while start < len(df):\n",
        "    start_time = time.time()\n",
        "    demo_df = df[start:start+limit].copy()\n",
        "    test_df = parallelize_dataframe(demo_df, extract_text_from_df, 94) #NUMBER OF CORES HERE\n",
        "    res.append(test_df)\n",
        "    test_df.to_csv(output_path, mode='a', header=not os.path.exists(output_path))\n",
        "\n",
        "    test_df['text_set'] = test_df['content'].map(lambda x: True if x[4] != '' else False)\n",
        "    no_of_text_retrieved = test_df.text_set.sum()\n",
        "    print(f'{no_of_text_retrieved} / {len(test_df)} text retrieved')\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f'Batch of data of row range {start}-{start+limit} complete in {round(end_time-start_time, 2)} seconds')\n",
        "    print(f'{round(min((((start+limit) / len(df)) * 100), 100), 2)}% complete')\n",
        "    start+=limit\n",
        "\n",
        "df = pd.concat(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ku5T0eTATz0"
      },
      "outputs": [],
      "source": [
        "output_path= 'unprocessed_' + csv_name\n",
        "if 'df' not in locals() or df is None or 'content' not in df:\n",
        "    if(os.path.exists(output_path)):\n",
        "#         df = pd.read_csv(output_path, on_bad_lines='skip', converters={'content': pd.eval}, index_col=0)\n",
        "        df = pd.read_csv(output_path, on_bad_lines='skip', index_col=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ez3HPeBmm8fm"
      },
      "source": [
        "# <font color='red'> If you're having problems extracting links (0 / 40 text retrieved) </font>\n",
        "\n",
        "### Double check in you browser the articles links you extracted above in 'link' column to see if they're working.\n",
        "### If they're not, correct your host you filled in above. You might have an extra '/' or repeated text in links such as '/es/es/news'\n",
        "\n",
        "## <font color='red'> If links are correct </font>\n",
        "### > Increase the sleep time of extract_urls_sleep in set up cell.\n",
        "### > So if it was random.randint(5,7), set it to random.randint(10,12)\n",
        "\n",
        "\n",
        "## <font color='red'> if issue still persist </font>\n",
        "### > Try to reduce number of cores in above cell from 94 to a lower number\n",
        "### For example, if you're getting 0 / 40 errors with 94 cores, lower it down to 50. If that doesn't work 25 and so on!\n",
        "\n",
        "# <font color='red'> If all that doesn't work and site is blocking you, just move on to next site and list reason/issue in excel file :) </font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyIWi9pCXzbJ"
      },
      "outputs": [],
      "source": [
        "df[['country', 'news_outlet', 'title', 'date', 'text']] = pd.DataFrame(df.content.tolist(), index= df.index)\n",
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAbNks4tOxpb"
      },
      "outputs": [],
      "source": [
        "print(len(df))\n",
        "df = df[df['text'] != '']\n",
        "print(len(df))\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKU62IqWQlYi"
      },
      "outputs": [],
      "source": [
        "if 'page' in df:\n",
        "  df = df.drop(['page'], axis=1)\n",
        "if 'content' in df:\n",
        "  df = df.drop(['content'], axis=1)\n",
        "if 'Unnamed: 0' in df:\n",
        "    df = df.drop(['Unnamed: 0'], axis=1)\n",
        "if 'Unnamed: 0.1' in df:\n",
        "    df = df.drop(['Unnamed: 0.1'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgKexrH4AawL"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNLqgUJHm8fm"
      },
      "source": [
        "# <font color='red'> IMPORTANT - UPDATE LINE 5 NEXT CELL WITH YOUR \"csv\" SAVE DIRECTORY </font>\n",
        "\n",
        "## Update this Path below to reflect your special workspace '/xdisk/josorio1/conflibert_esp/Summer_2023/Sultan/csv/'\n",
        "\n",
        "## >> For SPANISH <<\n",
        "### Go to https://ood.hpc.arizona.edu/pun/sys/dashboard/files/fs//xdisk/josorio1/conflibert_esp/Summer_2023\n",
        "### Find your directory and create folder named 'csv' inside of it.\n",
        "### Go inside 'csv' folder and click copy path and paste it below.\n",
        "### Make sure you close your copied path with \"/\"\n",
        "\n",
        "## >> For Arabic <<\n",
        "### Go to https://ood.hpc.arizona.edu/pun/sys/dashboard/files/fs//xdisk/josorio1/conflibert_ara/Summer_2023\n",
        "### Find your directory and create folder named 'csv' inside of it.\n",
        "### Go inside 'csv' folder and click copy path and paste it below.\n",
        "### Make sure you close your copied path with \"/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b8e0539"
      },
      "outputs": [],
      "source": [
        "#Please update\n",
        "seconds = time.time() - t0\n",
        "duration = time.strftime(\"(%H:%M:%S)\",time.gmtime(seconds))\n",
        "\n",
        "df.to_csv(csv_file_path+duration+'-'+csv_name, encoding = 'utf-8-sig')\n",
        "\n",
        "print('Time to complete:',duration)\n",
        "\n",
        "#tz_DFW = pytz.timezone('US/Central')\n",
        "#current_time = datetime.now(tz_DFW)\n",
        "#time = current_time.strftime(\"(%H:%M:%S)\")\n",
        "\n",
        "\n",
        "\n",
        "#df.to_csv(time+csv_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TJOmWM2dPI_"
      },
      "outputs": [],
      "source": [
        "if(os.path.exists(output_path)):\n",
        "    os.remove(output_path)\n",
        "    print(('unprocessed file deleted'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1C8RgfAdm8fn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}